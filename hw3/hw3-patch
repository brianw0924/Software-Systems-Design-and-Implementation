diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 1fc8faa6e973..4e7def488676 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -850,9 +850,11 @@ __SYSCALL(__NR_pidfd_open, sys_pidfd_open)
 #define __NR_clone3 435
 __SYSCALL(__NR_clone3, sys_clone3)
 #endif
+#define __NR_expose_pte 436
+__SYSCALL(__NR_expose_pte, sys_expose_pte)
 
 #undef __NR_syscalls
-#define __NR_syscalls 436
+#define __NR_syscalls 437
 
 /*
  * 32 bit systems traditionally used different
diff --git a/kernel/sys.c b/kernel/sys.c
index a611d1d58c7d..d69b8e5a69d1 100644
--- a/kernel/sys.c
+++ b/kernel/sys.c
@@ -4,7 +4,7 @@
  *
  *  Copyright (C) 1991, 1992  Linus Torvalds
  */
-
+#include <linux/expose_pte.h>
 #include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/utsname.h>
@@ -2581,6 +2581,91 @@ SYSCALL_DEFINE1(sysinfo, struct sysinfo __user *, info)
 	if (copy_to_user(info, &val, sizeof(struct sysinfo)))
 		return -EFAULT;
 
+	return 0;
+}
+
+SYSCALL_DEFINE1(expose_pte, struct expose_pte_args __user *, args)
+{
+
+	// int remap_pfn_range (
+	//	struct vm_area_struct * vma, (vma of the target addr)
+	// 	unsigned long addr, (target user addr)
+	// 	unsigned long pfn, (the physical addr from kernel memory)
+	// 	unsigned long size, (size you want to map, should align page size)
+	// 	pgprot_t prot
+	// );
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	int ret;
+	unsigned long i = 0;
+	struct task_struct *sheep_task = get_pid_task(find_get_pid(args->pid), PIDTYPE_PID);
+	struct task_struct *test_task = get_pid_task(find_get_pid(current->pid), PIDTYPE_PID);
+	struct mm_struct *sheep_mm = get_task_mm(sheep_task);
+	struct mm_struct *test_mm = get_task_mm(test_task);
+	struct vm_area_struct *test_vma;
+	unsigned long masked_begin_vaddr, masked_end_vaddr, pte_paddr;
+
+	// remap target PTE to remapped_pte
+
+	down_read(&sheep_mm->mmap_sem);
+	pr_info("#####################\n");
+	masked_begin_vaddr = args->begin_vaddr & PMD_MASK;
+	masked_end_vaddr = args->end_vaddr;
+	if ((masked_end_vaddr & ~PMD_MASK) == 0)
+		masked_end_vaddr = masked_begin_vaddr;
+	else
+		masked_end_vaddr = masked_end_vaddr & PMD_MASK;
+
+	for(; masked_begin_vaddr <= masked_end_vaddr; i++, masked_begin_vaddr+= (1 << PMD_SHIFT)) {
+		// pr_info("masked_begin_vaddr: %lx\n", masked_begin_vaddr);
+		test_vma = find_vma(test_mm, args->begin_pte_vaddr + i * PAGE_SIZE);
+
+		// walk through page table, find the pte physical addr
+		pgd = pgd_offset(sheep_mm, masked_begin_vaddr);
+		pud = pud_offset(pgd, masked_begin_vaddr);
+		pmd = pmd_offset(pud, masked_begin_vaddr);
+		pte = pte_offset_map(pmd, masked_begin_vaddr);
+		pte_paddr = pmd_page_paddr(READ_ONCE(*(pmd)));
+		// pr_info("PTE pfn: %lx\n", __phys_to_pfn(pmd_page_paddr(READ_ONCE(*(pmd)))));
+		// pr_info("PTE phys: %llx\n", pmd_page_paddr(READ_ONCE(*(pmd))));
+		if(pte_paddr) {
+			// remap PTE
+			// pfn 跟 phys 只差在 shift >> 12
+			ret = remap_pfn_range(
+				test_vma,
+				(args->begin_pte_vaddr + i * PAGE_SIZE),
+				__phys_to_pfn(pmd_page_paddr(READ_ONCE(*(pmd)))),
+				PAGE_SIZE,
+				test_vma->vm_page_prot
+				);
+			// pr_info("return value: %d\n", ret);
+			// fill in flattened table entry
+			*((unsigned long*)(args->begin_fpt_vaddr + i * 8)) = args->begin_pte_vaddr + i * PAGE_SIZE;
+		} else {
+			*((unsigned long*)(args->begin_fpt_vaddr + i * 8)) = 0;
+		}
+	}
+	i = 0;
+	for(; args->begin_vaddr + i * PAGE_SIZE < args->end_vaddr; i++) {
+		pgd = pgd_offset(sheep_mm, args->begin_vaddr + i * PAGE_SIZE);
+		pud = pud_offset(pgd, args->begin_vaddr + i * PAGE_SIZE);
+		pmd = pmd_offset(pud, args->begin_vaddr + i * PAGE_SIZE);
+		pte = pte_offset_map(pmd, args->begin_vaddr + i * PAGE_SIZE);
+		// pr_info("PTE pfn: %lx\n", __phys_to_pfn(pmd_page_paddr(READ_ONCE(*(pmd)))));
+		// pr_info("PTE phys: %llx\n", pmd_page_paddr(READ_ONCE(*(pmd))));
+		// pr_info("va: %lx pa: %llx\n", args->begin_vaddr + i * PAGE_SIZE, pte_val(*pte));
+
+	}
+	pr_info("#####################\n");
+	up_read(&sheep_mm->mmap_sem);
+
+	
+	
+
+
+
 	return 0;
 }
 
